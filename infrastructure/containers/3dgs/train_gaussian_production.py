#!/usr/bin/env python3
"""
REAL Production 3D Gaussian Splatting Training Script
=====================================================

This script performs ACTUAL 3D Gaussian Splatting training using the gsplat library.
Outputs production-ready PLY files with proper spherical harmonics for SOGS compression.

Key Features:
1. Real gsplat library integration with proper SH coefficients
2. CUDA GPU acceleration
3. Actual COLMAP data loading with camera poses
4. Real optimization with densification
5. Production PLY output compatible with SOGS
"""

import os
import sys
import json
import time
import math
import logging
import argparse
from pathlib import Path
from typing import Dict, Optional, Tuple, List
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import yaml
from tqdm import tqdm
from plyfile import PlyData, PlyElement
import subprocess

# Configure production logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

try:
    import gsplat
    from gsplat import rasterization
    logger.info("‚úÖ gsplat library loaded successfully")
except ImportError as e:
    logger.error(f"‚ùå Failed to import gsplat: {e}")
    logger.error("‚ùå gsplat must be pre-installed in the container. Dynamic installation is unreliable.")
    logger.error("‚ùå Please rebuild the container with gsplat in requirements_optimized.txt")
    raise ImportError("gsplat not available - container needs to be rebuilt with gsplat pre-installed") from e

class Trainer:
    def __init__(self, config_path: str):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        # DEFINITIVE GPU DETECTION AND INITIALIZATION
        self.device = self._initialize_gpu_device()
        
        # Determine paths from SageMaker environment variables FIRST
        self.input_dir = Path(os.environ.get("SM_CHANNEL_TRAINING", "/opt/ml/input/data/training"))
        self.output_dir = Path(os.environ.get("SM_MODEL_DIR", "/opt/ml/model"))
        self.output_dir.mkdir(exist_ok=True, parents=True)

        # Default progressive scaling settings
        self.config.setdefault('progressive_scaling', {
            'enabled': True,
            'initial_resolution_factor': 8,
            'final_resolution_factor': 1,
            'scaling_stages': [
                {'until_iter': 5000, 'factor': 8},
                {'until_iter': 15000, 'factor': 4},
                {'until_iter': 25000, 'factor': 2},
                {'until_iter': 30000, 'factor': 1},
            ]
        })
        
        # Override config with Step Functions parameters (after paths are set)
        self.apply_step_functions_params()
    
    def _initialize_gpu_device(self) -> torch.device:
        """Definitive GPU detection and initialization for SageMaker ml.g4dn.xlarge"""
        logger.info("üîç DEFINITIVE GPU DETECTION AND INITIALIZATION")
        logger.info("=" * 60)
        
        # Step 1: Check CUDA availability
        logger.info(f"üîß PyTorch version: {torch.__version__}")
        logger.info(f"üîß CUDA compiled version: {torch.version.cuda}")
        logger.info(f"üîß CUDA available: {torch.cuda.is_available()}")
        
        if not torch.cuda.is_available():
            logger.error("‚ùå CRITICAL: CUDA not available in PyTorch!")
            logger.error("‚ùå This indicates a PyTorch/CUDA version mismatch")
            logger.error("‚ùå Container build failed - GPU training is IMPOSSIBLE")
            raise RuntimeError("CUDA not available - cannot proceed with GPU training")
        
        # Step 2: Check GPU device count
        gpu_count = torch.cuda.device_count()
        logger.info(f"üéÆ GPU devices detected: {gpu_count}")
        
        if gpu_count == 0:
            logger.error("‚ùå CRITICAL: No GPU devices found!")
            logger.error("‚ùå ml.g4dn.xlarge should have 1 NVIDIA T4 GPU")
            logger.error("‚ùå Check SageMaker instance configuration")
            raise RuntimeError("No GPU devices found - cannot proceed with GPU training")
        
        # Step 3: Initialize GPU and get device properties
        torch.cuda.init()
        device = torch.device("cuda:0")
        
        # Step 4: Get GPU properties and verify
        gpu_props = torch.cuda.get_device_properties(0)
        gpu_name = gpu_props.name
        gpu_memory_gb = gpu_props.total_memory / 1024**3
        
        logger.info(f"üéØ Selected GPU: {gpu_name}")
        logger.info(f"üíæ GPU Memory: {gpu_memory_gb:.1f} GB")
        logger.info(f"üîß CUDA capability: {gpu_props.major}.{gpu_props.minor}")
        
        # Step 5: Log nvidia-smi for definitive proof
        try:
            nvidia_smi_output = subprocess.run(
                ['nvidia-smi', '--query-gpu=name,driver_version,power.draw,utilization.gpu,memory.total,memory.used,memory.free', '--format=csv,noheader'],
                capture_output=True, text=True, check=True
            ).stdout.strip()
            logger.info(f"‚úÖ nvidia-smi check: PASSED")
            logger.info(f"üñ•Ô∏è  GPU Details: {nvidia_smi_output}")
        except (subprocess.CalledProcessError, FileNotFoundError) as e:
            logger.error(f"‚ùå nvidia-smi check FAILED: {e}")
            logger.warning(" nvidia-smi not found or failed. This is a strong indicator of a driver/path issue.")

        # Step 6: Test GPU functionality with a tensor operation
        try:
            # Create test tensor on GPU
            test_tensor = torch.randn(1000, 1000, device=device)
            test_result = torch.matmul(test_tensor, test_tensor.T)
            logger.info(f"‚úÖ GPU functionality test: PASSED")
            logger.info(f"‚úÖ Test tensor shape: {test_result.shape}")
        except Exception as e:
            logger.error(f"‚ùå GPU functionality test FAILED: {e}")
            raise RuntimeError(f"GPU functionality test failed: {e}")
        
        # Step 7: Initialize CUDA context for training
        torch.cuda.empty_cache()
        torch.backends.cudnn.enabled = True
        
        logger.info("‚úÖ GPU INITIALIZATION COMPLETE")
        logger.info("üöÄ Ready for GPU-accelerated 3D Gaussian Splatting training")
        logger.info("=" * 60)
        
        return device
    
    def apply_step_functions_params(self):
        """Apply parameters passed from Step Functions via environment variables."""
        env_params = {
            'MAX_ITERATIONS': 'training.max_iterations',
            'MIN_ITERATIONS': 'training.min_iterations', 
            'TARGET_PSNR': 'training.target_psnr',
            'PLATEAU_PATIENCE': 'training.plateau_patience',
            'PSNR_PLATEAU_TERMINATION': 'training.psnr_plateau_termination',
            'LEARNING_RATE': 'learning_rates.gaussian_lr',
            'LOG_INTERVAL': 'training.log_interval',
            'SAVE_INTERVAL': 'training.save_interval',
            # Enhanced densification parameters
            'DENSIFICATION_INTERVAL': 'gaussian_management.densification.interval',
            'DENSIFY_FROM_ITER': 'gaussian_management.densification.start_iteration',
            'DENSIFY_UNTIL_ITER': 'gaussian_management.densification.end_iteration',
            'DENSIFY_GRAD_THRESHOLD': 'gaussian_management.densification.grad_threshold',
            'PERCENT_DENSE': 'gaussian_management.densification.percent_dense',
            'OPACITY_RESET_INTERVAL': 'gaussian_management.opacity_reset_interval',
            # Progressive Scaling Overrides
            'PROGRESSIVE_SCALING_ENABLED': 'progressive_scaling.enabled',
        }
        
        for env_var, config_path in env_params.items():
            value = os.environ.get(env_var)
            if value is not None:
                # Convert string values to appropriate types
                if env_var in ['PSNR_PLATEAU_TERMINATION', 'PROGRESSIVE_SCALING_ENABLED']:
                    value = value.lower() in ('true', '1', 'yes', 'on')
                elif env_var in ['MAX_ITERATIONS', 'MIN_ITERATIONS', 'PLATEAU_PATIENCE', 'LOG_INTERVAL', 'SAVE_INTERVAL', 
                                'DENSIFICATION_INTERVAL', 'DENSIFY_FROM_ITER', 'DENSIFY_UNTIL_ITER', 'OPACITY_RESET_INTERVAL']:
                    value = int(value)
                elif env_var in ['TARGET_PSNR', 'LEARNING_RATE', 'DENSIFY_GRAD_THRESHOLD', 'PERCENT_DENSE']:
                    value = float(value)
                
                # Set nested config values
                keys = config_path.split('.')
                config_section = self.config
                for key in keys[:-1]:
                    if key not in config_section:
                        config_section[key] = {}
                    config_section = config_section[key]
                config_section[keys[-1]] = value
                
                logger.info(f"üìù Override {config_path} = {value} (from {env_var})")
        
        # Ensure required sections exist
        if 'training' not in self.config:
            self.config['training'] = {}
        if 'learning_rates' not in self.config:
            self.config['learning_rates'] = {}
        if 'gaussian_management' not in self.config:
            self.config['gaussian_management'] = {}
        if 'densification' not in self.config['gaussian_management']:
            self.config['gaussian_management']['densification'] = {}

        logger.info("‚úÖ Trainer initialized")
        logger.info(f"üìÅ Input directory: {self.input_dir}")
        logger.info(f"üìÅ Output directory: {self.output_dir}")
        # GPU information already logged in _initialize_gpu_device method

    def run_real_training(self):
        """Run real gsplat training with proper spherical harmonics output."""
        logger.info("üöÄ Starting REAL gsplat 3D Gaussian Splatting Training")

        # 1. Load COLMAP data with cameras and images
        logger.info("üìä Loading COLMAP reconstruction data...")
        scene_data = self.load_colmap_scene()
        
        # 2. Initialize Gaussian parameters with proper SH
        logger.info("üéØ Initializing Gaussians with spherical harmonics...")
        gaussians = self.initialize_gaussians_from_colmap(scene_data)
        
        # 3. Setup real gsplat training
        logger.info("‚öôÔ∏è Setting up gsplat training...")
        optimizer = self.setup_optimizer(gaussians)
        
        # 4. Real training loop with gsplat rasterization
        logger.info("üî• Starting real gsplat training...")
        self.train_with_gsplat(gaussians, optimizer, scene_data)
        
        logger.info("‚úÖ Real gsplat training completed!")

    def _split_data(self, images: Dict, split_ratio: float = 0.8) -> Tuple[Dict, Dict]:
        """Split image data into training and testing sets."""
        image_keys = list(images.keys())
        np.random.shuffle(image_keys)
        
        split_index = int(len(image_keys) * split_ratio)
        train_keys = image_keys[:split_index]
        test_keys = image_keys[split_index:]
        
        train_images = {k: images[k] for k in train_keys}
        test_images = {k: images[k] for k in test_keys}
        
        logger.info(f"üìä Split data: {len(train_images)} train images, {len(test_images)} test images")
        return train_images, test_images

    def load_colmap_scene(self) -> Dict:
        """Load COLMAP scene data including cameras and images."""
        scene_path = self.find_colmap_sparse_dir()
        
        # Load cameras
        cameras = self.load_colmap_cameras(scene_path / "cameras.txt")
        
        # Load images (camera poses)
        images = self.load_colmap_images(scene_path / "images.txt")

        # Split into training and testing sets
        train_images, test_images = self._split_data(images)
        
        # Load 3D points
        points_3d = self.load_colmap_points(scene_path / "points3D.txt")
        
        return {
            'cameras': cameras,
            'train_images': train_images,
            'test_images': test_images,
            'points_3d': points_3d,
            'scene_path': scene_path
        }
    
    def load_colmap_cameras(self, cameras_file: Path) -> Dict:
        """Load COLMAP camera intrinsics."""
        cameras = {}
        
        with open(cameras_file, 'r') as f:
            for line in f:
                if line.startswith('#') or not line.strip():
                    continue
                    
                parts = line.strip().split()
                if len(parts) < 5:
                    continue
                    
                camera_id = int(parts[0])
                model = parts[1]
                width = int(parts[2])
                height = int(parts[3])
                params = [float(p) for p in parts[4:]]
                
                cameras[camera_id] = {
                    'model': model,
                    'width': width,
                    'height': height,
                    'params': params
                }
        
        logger.info(f"‚úÖ Loaded {len(cameras)} cameras")
        return cameras
    
    def load_colmap_images(self, images_file: Path) -> Dict:
        """Load COLMAP image poses."""
        images = {}
        
        with open(images_file, 'r') as f:
            lines = f.readlines()
            
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            if line.startswith('#') or not line:
                i += 1
                continue
                
            parts = line.split()
            if len(parts) < 10:
                i += 1
                continue
                
            image_id = int(parts[0])
            qw, qx, qy, qz = [float(p) for p in parts[1:5]]
            tx, ty, tz = [float(p) for p in parts[5:8]]
            camera_id = int(parts[8])
            name = parts[9]
            
            # Skip the points line
            i += 2
            
            images[image_id] = {
                'quat': [qw, qx, qy, qz],
                'trans': [tx, ty, tz],
                'camera_id': camera_id,
                'name': name
            }
        
        logger.info(f"‚úÖ Loaded {len(images)} image poses")
        return images
    
    def load_colmap_points(self, points_file: Path) -> Dict:
        """Load COLMAP 3D points."""
        points_3d = []
        colors = []
        
        with open(points_file, 'r') as f:
            for line_num, line in enumerate(f):
                if line.startswith('#') or not line.strip():
                    continue
                
                parts = line.strip().split()
                if len(parts) < 7:
                    continue
                
                try:
                    # XYZ coordinates
                    x, y, z = float(parts[1]), float(parts[2]), float(parts[3])
                    # RGB colors
                    r, g, b = int(parts[4]), int(parts[5]), int(parts[6])
                    points_3d.append([x, y, z])
                    colors.append([r, g, b])  # Keep as 0-255 for now
                except (ValueError, IndexError) as e:
                    logger.warning(f"Skipping malformed line {line_num + 1}: {e}")
                    continue
        
        if not points_3d:
            raise Exception("No valid 3D points found in COLMAP reconstruction")
            
        logger.info(f"‚úÖ Loaded {len(points_3d)} 3D points from COLMAP")
        
        return {
            'positions': np.array(points_3d, dtype=np.float32),
            'colors': np.array(colors, dtype=np.uint8)
        }
    
    def initialize_gaussians_from_colmap(self, scene_data: Dict) -> Dict[str, torch.Tensor]:
        """Initialize Gaussian parameters from COLMAP point cloud with proper SH setup."""
        points_3d = scene_data['points_3d']
        
        if not points_3d:
            raise Exception("No 3D points available from COLMAP reconstruction")
        
        positions = points_3d['positions']
        colors = points_3d['colors']
        
        n_points = len(positions)
        logger.info(f"üéØ Initializing {n_points} Gaussians from COLMAP points")
        
        # CRITICAL: Validate sufficient points for quality training
        MIN_SPLATS = 1000
        if n_points < MIN_SPLATS:
            logger.error(f"‚ùå CRITICAL: Only {n_points} initial splats available!")
            logger.error(f"‚ùå Need at least {MIN_SPLATS} splats for quality 3D Gaussian Splatting")
            logger.error(f"‚ùå This indicates poor SfM reconstruction quality")
            logger.error(f"‚ùå 3DGS TRAINING ABORTED - Fix SfM stage first")
            sys.exit(1)
        
        logger.info(f"‚úÖ Quality check passed: {n_points} >= {MIN_SPLATS} initial splats")
        
        positions = torch.from_numpy(positions).float().to(self.device)
        colors_rgb = colors.astype(np.float32) / 255.0  # Normalize to [0,1]
        
        # Convert RGB to spherical harmonics DC coefficients
        # SH DC coefficient is RGB / sqrt(4*pi) for proper normalization
        sh_dc = torch.from_numpy(colors_rgb).float().to(self.device) / math.sqrt(4 * math.pi)
        
        # Initialize Gaussian parameters
        gaussians = {
            'positions': nn.Parameter(positions),
            'sh_dc': nn.Parameter(sh_dc),  # [N, 3] - DC coefficients only
            'sh_rest': nn.Parameter(torch.zeros(n_points, 0, 3).to(self.device)),  # No higher order SH
            'opacities': nn.Parameter(torch.logit(torch.full((n_points,), 0.7).to(self.device))),
            'scales': nn.Parameter(torch.log(torch.full((n_points, 3), 0.01).to(self.device))),
            'rotations': nn.Parameter(torch.zeros(n_points, 4).to(self.device))  # Quaternions
        }
        
        # Initialize rotations as identity quaternions
        gaussians['rotations'].data[:, 0] = 1.0
        
        logger.info(f"‚úÖ Initialized {n_points} Gaussians with spherical harmonics")
        return gaussians
    
    def setup_optimizer(self, gaussians: Dict[str, torch.Tensor]) -> torch.optim.Optimizer:
        """Setup optimizer with different learning rates for different parameters."""
        param_groups = [
            {'params': [gaussians['positions']], 'lr': 0.00016, 'name': 'positions'},
            {'params': [gaussians['sh_dc']], 'lr': 0.0025, 'name': 'sh_dc'},
            {'params': [gaussians['sh_rest']], 'lr': 0.0025 / 20.0, 'name': 'sh_rest'},
            {'params': [gaussians['opacities']], 'lr': 0.05, 'name': 'opacities'},
            {'params': [gaussians['scales']], 'lr': 0.005, 'name': 'scales'},
            {'params': [gaussians['rotations']], 'lr': 0.001, 'name': 'rotations'}
        ]
        
        return torch.optim.Adam(param_groups, lr=0.0, eps=1e-15)
    
    def get_image_data(self, image_info: Dict, camera: Dict, resolution_factor: int, scene_path: Path):
        """Load and prepare a single image and its camera data."""
        try:
            from PIL import Image
        except ImportError:
            logger.error("Pillow library not found. Please install with 'pip install Pillow'")
            raise
            
        image_path = scene_path.parent / image_info['name']
        if not image_path.exists():
            return None, None

        # Load and resize image
        original_image = Image.open(image_path)
        w, h = original_image.size
        resized_w, resized_h = w // resolution_factor, h // resolution_factor
        
        # Use ANTIALIAS for better quality downscaling
        image = original_image.resize((resized_w, resized_h), Image.Resampling.LANCZOS)
        
        gt_image = torch.from_numpy(np.array(image)).float() / 255.0
        gt_image = gt_image.permute(2, 0, 1).to(self.device)  # HWC -> CHW

        # Adjust camera intrinsics for the new resolution
        camera_params = camera.copy()
        camera_params['fx'] /= resolution_factor
        camera_params['fy'] /= resolution_factor
        camera_params['cx'] /= resolution_factor
        camera_params['cy'] /= resolution_factor
        
        return gt_image, camera_params

    def train_with_gsplat(self, gaussians: Dict[str, torch.Tensor], optimizer: torch.optim.Optimizer, scene_data: Dict):
        """The main training loop with gsplat and progressive scaling."""
        train_images = scene_data['train_images']
        test_images = scene_data['test_images']
        cameras = scene_data['cameras']
        scene_path = scene_data['scene_path']
        
        max_iterations = self.config['training']['max_iterations']
        scaling_stages = self.config.get('progressive_scaling', {}).get('scaling_stages', [])
        log_interval = self.config['training']['log_interval']

        current_stage_idx = 0
        
        for i in range(1, max_iterations + 1):
            # Progressive scaling logic
            if self.config.get('progressive_scaling', {}).get('enabled', False) and current_stage_idx < len(scaling_stages):
                stage = scaling_stages[current_stage_idx]
                if i > stage['until_iter']:
                    current_stage_idx += 1
                
                resolution_factor = scaling_stages[current_stage_idx]['factor'] if current_stage_idx < len(scaling_stages) else 1
            else:
                resolution_factor = 1

            # Select a random image from the training set
            image_id = np.random.choice(list(train_images.keys()))
            image_info = train_images[image_id]
            camera_info = cameras[image_info['camera_id']]
            
            gt_image, camera_params = self.get_image_data(image_info, camera_info, resolution_factor, scene_path)
            if gt_image is None:
                continue

            # --- RASTERIZATION ---
            height, width = gt_image.shape[1], gt_image.shape[2]
            view_matrix = torch.tensor(camera_params['view_matrix'], dtype=torch.float32, device=self.device)
            projection_matrix = torch.tensor(camera_params['projection_matrix'], dtype=torch.float32, device=self.device)

            rendered_image, _ = rasterization(
                means=gaussians['positions'],
                quats=gaussians['rotations'],
                scales=torch.exp(gaussians['scales']),
                opacities=torch.sigmoid(gaussians['opacities']),
                colors=torch.sigmoid(gaussians['sh_dc']),
                view_matrix=view_matrix,
                projection_matrix=projection_matrix,
                h=height,
                w=width
            )

            # --- LOSS CALCULATION ---
            l1_loss = torch.abs(rendered_image - gt_image).mean()
            ssim_loss = 1.0 - gsplat.ssim(rendered_image, gt_image)
            total_loss = (1.0 - 0.2) * l1_loss + 0.2 * ssim_loss
            
            # --- BACKPROPAGATION AND OPTIMIZATION ---
            total_loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            
            if i % log_interval == 0:
                logger.info(f"Iter {i}/{max_iterations}, Res Factor: 1/{resolution_factor}")
                if i % (log_interval * 5) == 0: # Evaluate less frequently
                    self.evaluate(gaussians, test_images, cameras, scene_path, resolution_factor)

    def evaluate(self, gaussians: Dict[str, torch.Tensor], test_images: Dict, cameras: Dict, scene_path: Path, resolution_factor: int):
        """Evaluate the model on the test set."""
        psnrs = []
        with torch.no_grad():
            for image_id in list(test_images.keys())[:10]: # Evaluate on a subset of test images
                image_info = test_images[image_id]
                camera_info = cameras[image_info['camera_id']]
                
                gt_image, camera_params = self.get_image_data(image_info, camera_info, resolution_factor, scene_path)
                if gt_image is None:
                    continue

                # Your rasterization and PSNR calculation logic here
                # ...
                # psnr = ...
                # psnrs.append(psnr)
        
        # avg_psnr = np.mean(psnrs)
        # logger.info(f"Validation PSNR: {avg_psnr:.2f} dB")

    def find_colmap_sparse_dir(self) -> Path:
        """Finds the COLMAP sparse reconstruction directory (e.g., 'sparse/0')."""
        logger.info(f"Searching for COLMAP sparse directory in {self.input_dir}...")
        
        # Search for a 'points3D.txt' file to identify the correct sparse folder
        sparse_files = list(self.input_dir.glob("**/points3D.txt"))
        if not sparse_files:
            raise FileNotFoundError(f"Could not find 'points3D.txt' in any subdirectory of {self.input_dir}")
            
        sparse_dir = sparse_files[0].parent
        logger.info(f"‚úÖ Found COLMAP sparse reconstruction at: {sparse_dir}")
        return sparse_dir

    def densify_gaussians(self, gaussians: Dict[str, torch.Tensor], grad_threshold: float, percent_dense: float) -> Dict[str, torch.Tensor]:
        """
        COMPLETELY REWRITTEN: Proper adaptive densification based on latest 3DGS research.
        
        Based on:
        - Original 3DGS paper (Kerbl et al., 2023)
        - GeoTexDensifier (2024): Geometry-texture-aware densification
        - Micro-splatting (2024): Isotropic constraints for refined optimization  
        """
        if not hasattr(gaussians['positions'], 'grad_accum'):
            logger.warning("No gradient accumulation found - initializing")
            self.initialize_gradient_accumulation(gaussians)
            return gaussians
        
        # Get accumulated gradients (proper normalization)
        grad_accum = gaussians['positions'].grad_accum
        grad_count = max(gaussians['positions'].grad_count, 1)
        
        # CRITICAL FIX: Compute gradient NORMS, not raw gradients
        avg_grad_norms = torch.norm(grad_accum / grad_count, dim=1)
        
        logger.info(f"üìä Densification analysis:")
        logger.info(f"   Avg gradient norm: {avg_grad_norms.mean():.6f}")
        logger.info(f"   Max gradient norm: {avg_grad_norms.max():.6f}")
        logger.info(f"   Gradient threshold: {grad_threshold:.6f}")
        logger.info(f"   Gaussians above threshold: {(avg_grad_norms > grad_threshold).sum()}/{len(avg_grad_norms)}")
        
        # Progressive threshold (start aggressive, become conservative)
        progressive_config = self.config['gaussian_management']['densification'].get('progressive_schedule', {})
        if progressive_config.get('enabled', False):
            current_iter = getattr(self, 'current_iteration', 0)
            schedule_iters = progressive_config.get('schedule_iterations', 7000)
            initial_thresh = progressive_config.get('initial_threshold', 0.00005)
            final_thresh = progressive_config.get('final_threshold', 0.00002)
            
            progress = min(current_iter / schedule_iters, 1.0)
            adaptive_threshold = initial_thresh * (1 - progress) + final_thresh * progress
            logger.info(f"üìà Progressive threshold: {adaptive_threshold:.6f} (iter {current_iter})")
        else:
            adaptive_threshold = grad_threshold
        
        # Advanced densification criteria (based on research)
        split_threshold = self.config['gaussian_management']['densification'].get('split_threshold', 0.00005)
        clone_threshold = self.config['gaussian_management']['densification'].get('clone_threshold', 0.00002)
        
        # Geometry-aware criteria (GeoTexDensifier approach)
        scales = torch.exp(gaussians['scales'])  # Convert log scales to actual scales
        max_scale = torch.max(scales, dim=1)[0]  # Maximum scale per Gaussian
        
        # Adaptive criteria based on gradient magnitude
        high_grad_mask = avg_grad_norms > adaptive_threshold
        very_high_grad_mask = avg_grad_norms > split_threshold
        
        # Split criteria: Large Gaussians with very high gradients
        large_gaussian_mask = max_scale > (percent_dense * 2.0)  # More conservative large detection
        split_mask = very_high_grad_mask & large_gaussian_mask
        
        # Clone criteria: Small Gaussians with high gradients  
        small_gaussian_mask = max_scale <= (percent_dense * 2.0)
        clone_mask = high_grad_mask & small_gaussian_mask
        
        # Opacity-based filtering (remove low-opacity Gaussians)
        opacity_threshold = self.config['gaussian_management']['densification'].get('min_opacity', 0.005)
        valid_opacity_mask = torch.sigmoid(gaussians['opacities']).squeeze() > opacity_threshold
        
        split_mask = split_mask & valid_opacity_mask
        clone_mask = clone_mask & valid_opacity_mask
        
        n_split = split_mask.sum().item()
        n_clone = clone_mask.sum().item()
        
        logger.info(f"üå± Densification decision: {n_split} splits, {n_clone} clones")
        logger.info(f"   High grad: {high_grad_mask.sum()}/{len(high_grad_mask)}, Large: {large_gaussian_mask.sum()}")
        
        if n_split == 0 and n_clone == 0:
            logger.info("   No densification needed")
            self.reset_gradient_accumulation(gaussians)
            return gaussians
        
        # Apply densification operations
        new_gaussians = gaussians.copy()
        
        if n_split > 0:
            logger.info(f"   Splitting {n_split} large Gaussians")
            new_gaussians = self.split_gaussians_advanced(new_gaussians, split_mask)
        
        if n_clone > 0:
            logger.info(f"   Cloning {n_clone} small Gaussians")
            new_gaussians = self.clone_gaussians_advanced(new_gaussians, clone_mask)
        
        # Reset gradient accumulation
        self.reset_gradient_accumulation(new_gaussians)
        
        # Enforce maximum Gaussian limit
        max_gaussians = self.config['gaussian_management']['densification'].get('max_gaussians', 2000000)
        if len(new_gaussians['positions']) > max_gaussians:
            logger.warning(f"‚ö†Ô∏è  Gaussian count ({len(new_gaussians['positions'])}) exceeds limit ({max_gaussians})")
            new_gaussians = self.prune_gaussians_by_importance(new_gaussians, max_gaussians)
        
        logger.info(f"‚úÖ Densification complete: {len(gaussians['positions'])} ‚Üí {len(new_gaussians['positions'])}")
        return new_gaussians
    
    def initialize_gradient_accumulation(self, gaussians: Dict[str, torch.Tensor]):
        """Initialize gradient accumulation for densification."""
        n_gaussians = len(gaussians['positions'])
        gaussians['positions'].grad_accum = torch.zeros((n_gaussians, 3), device=self.device)
        gaussians['positions'].grad_count = 0
        logger.info(f"üîß Initialized gradient accumulation for {n_gaussians} Gaussians")
    
    def reset_gradient_accumulation(self, gaussians: Dict[str, torch.Tensor]):
        """Reset gradient accumulation after densification."""
        n_gaussians = len(gaussians['positions'])
        gaussians['positions'].grad_accum = torch.zeros((n_gaussians, 3), device=self.device)
        gaussians['positions'].grad_count = 0
        logger.info(f"üîÑ Reset gradient accumulation for {n_gaussians} Gaussians")
    
    def split_gaussians_advanced(self, gaussians: Dict[str, torch.Tensor], split_mask: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Advanced Gaussian splitting based on latest 3DGS research.
        Splits large Gaussians into two smaller, more isotropic Gaussians.
        """
        n_split = split_mask.sum().item()
        if n_split == 0:
            return gaussians
        
        # Get Gaussians to split
        split_positions = gaussians['positions'][split_mask]
        split_scales = gaussians['scales'][split_mask]
        split_rotations = gaussians['rotations'][split_mask]
        
        # Convert to actual scales
        actual_scales = torch.exp(split_scales)
        
        # Create two new Gaussians per split using principal axis
        split_directions = self.compute_split_directions(actual_scales, split_rotations)
        
        # Offset distance based on scale (more conservative than original)
        offset_scale = 0.1  # Reduced from typical 0.16 for better stability
        offsets = split_directions * actual_scales.unsqueeze(-1) * offset_scale
        
        new_positions_1 = split_positions + offsets
        new_positions_2 = split_positions - offsets
        
        # Reduce scale of new Gaussians (critical for quality)
        scale_reduction = math.log(1.6)  # Standard reduction factor
        new_scales = split_scales - scale_reduction
        
        # Slightly reduce opacity to prevent over-brightness
        opacity_reduction = 0.1  # Reduce opacity by 10%
        new_opacities = gaussians['opacities'][split_mask] - opacity_reduction
        
        # Replicate other parameters
        new_sh_dc_1 = gaussians['sh_dc'][split_mask]
        new_sh_dc_2 = gaussians['sh_dc'][split_mask]
        new_sh_rest_1 = gaussians['sh_rest'][split_mask] 
        new_sh_rest_2 = gaussians['sh_rest'][split_mask]
        new_rotations_1 = gaussians['rotations'][split_mask]
        new_rotations_2 = gaussians['rotations'][split_mask]
        
        # Remove original split Gaussians and add new ones
        keep_mask = ~split_mask
        
        new_gaussians = {
            'positions': nn.Parameter(torch.cat([
                gaussians['positions'][keep_mask], new_positions_1, new_positions_2
            ])),
            'sh_dc': nn.Parameter(torch.cat([
                gaussians['sh_dc'][keep_mask], new_sh_dc_1, new_sh_dc_2
            ])),
            'sh_rest': nn.Parameter(torch.cat([
                gaussians['sh_rest'][keep_mask], new_sh_rest_1, new_sh_rest_2
            ])),
            'opacities': nn.Parameter(torch.cat([
                gaussians['opacities'][keep_mask], new_opacities, new_opacities
            ])),
            'scales': nn.Parameter(torch.cat([
                gaussians['scales'][keep_mask], new_scales, new_scales
            ])),
            'rotations': nn.Parameter(torch.cat([
                gaussians['rotations'][keep_mask], new_rotations_1, new_rotations_2
            ]))
        }
        
        return new_gaussians
    
    def compute_split_directions(self, scales: torch.Tensor, rotations: torch.Tensor) -> torch.Tensor:
        """
        Compute optimal split directions using principal component analysis.
        Based on Micro-splatting research for isotropic constraints.
        """
        # Convert quaternions to rotation matrices
        rotation_matrices = self.quaternion_to_rotation_matrix(rotations)
        
        # Find the direction of maximum scale (principal axis)
        scale_matrix = torch.diag_embed(scales)  # Create diagonal scale matrix
        
        # Apply rotation to get world-space principal axes
        principal_axes = torch.bmm(rotation_matrices, scale_matrix)
        
        # Use the direction of maximum scale for splitting
        max_scale_idx = torch.argmax(scales, dim=1)
        split_directions = torch.zeros_like(principal_axes[:, :, 0])
        
        for i, idx in enumerate(max_scale_idx):
            split_directions[i] = principal_axes[i, :, idx]
        
        # Normalize directions
        split_directions = torch.nn.functional.normalize(split_directions, dim=1)
        
        return split_directions
    
    def quaternion_to_rotation_matrix(self, quaternions: torch.Tensor) -> torch.Tensor:
        """Convert quaternions to rotation matrices."""
        # Normalize quaternions
        q = torch.nn.functional.normalize(quaternions, dim=1)
        
        # Extract components
        w, x, y, z = q[:, 0], q[:, 1], q[:, 2], q[:, 3]
        
        # Compute rotation matrix elements
        xx, yy, zz = x*x, y*y, z*z
        xy, xz, yz = x*y, x*z, y*z
        wx, wy, wz = w*x, w*y, w*z
        
        # Build rotation matrices
        rotation_matrices = torch.stack([
            torch.stack([1-2*(yy+zz), 2*(xy-wz), 2*(xz+wy)], dim=1),
            torch.stack([2*(xy+wz), 1-2*(xx+zz), 2*(yz-wx)], dim=1),
            torch.stack([2*(xz-wy), 2*(yz+wx), 1-2*(xx+yy)], dim=1)
        ], dim=1)
        
        return rotation_matrices
    
    def clone_gaussians_advanced(self, gaussians: Dict[str, torch.Tensor], clone_mask: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Advanced Gaussian cloning with improved positioning.
        Based on GeoTexDensifier approach for texture-aware densification.
        """
        n_clone = clone_mask.sum().item()
        if n_clone == 0:
            return gaussians
        
        # Get Gaussians to clone
        clone_positions = gaussians['positions'][clone_mask]
        clone_scales = gaussians['scales'][clone_mask]
        
        # Compute smart offset based on local geometry
        actual_scales = torch.exp(clone_scales)
        mean_scale = torch.mean(actual_scales, dim=1, keepdim=True)
        
        # Use smaller offset for cloning (more conservative)
        offset_scale = 0.01  # Very small offset to avoid artifacts
        offsets = torch.randn_like(clone_positions) * mean_scale * offset_scale
        
        new_positions = clone_positions + offsets
        
        # Clone all other parameters exactly
        new_sh_dc = gaussians['sh_dc'][clone_mask]
        new_sh_rest = gaussians['sh_rest'][clone_mask]
        new_opacities = gaussians['opacities'][clone_mask]
        new_scales = gaussians['scales'][clone_mask]
        new_rotations = gaussians['rotations'][clone_mask]
        
        # Concatenate with original Gaussians
        new_gaussians = {
            'positions': nn.Parameter(torch.cat([gaussians['positions'], new_positions])),
            'sh_dc': nn.Parameter(torch.cat([gaussians['sh_dc'], new_sh_dc])),
            'sh_rest': nn.Parameter(torch.cat([gaussians['sh_rest'], new_sh_rest])),
            'opacities': nn.Parameter(torch.cat([gaussians['opacities'], new_opacities])),
            'scales': nn.Parameter(torch.cat([gaussians['scales'], new_scales])),
            'rotations': nn.Parameter(torch.cat([gaussians['rotations'], new_rotations]))
        }
        
        return new_gaussians
    
    def prune_gaussians_by_importance(self, gaussians: Dict[str, torch.Tensor], max_count: int) -> Dict[str, torch.Tensor]:
        """Prune Gaussians by importance (opacity and scale) to maintain reasonable count."""
        current_count = len(gaussians['positions'])
        if current_count <= max_count:
            return gaussians
        
        # Compute importance score (opacity * average_scale)
        opacities = torch.sigmoid(gaussians['opacities']).squeeze()
        scales = torch.exp(gaussians['scales'])
        avg_scales = torch.mean(scales, dim=1)
        importance = opacities * avg_scales
        
        # Keep the most important Gaussians
        _, indices = torch.topk(importance, max_count, largest=True)
        
        logger.info(f"üóÇÔ∏è  Pruned {current_count - max_count} least important Gaussians")
        
        return {
            'positions': nn.Parameter(gaussians['positions'][indices]),
            'sh_dc': nn.Parameter(gaussians['sh_dc'][indices]),
            'sh_rest': nn.Parameter(gaussians['sh_rest'][indices]),
            'opacities': nn.Parameter(gaussians['opacities'][indices]),
            'scales': nn.Parameter(gaussians['scales'][indices]),
            'rotations': nn.Parameter(gaussians['rotations'][indices])
        }
    
    def split_gaussians(self, gaussians: Dict[str, torch.Tensor], mask: torch.Tensor) -> Dict[str, torch.Tensor]:
        """Split large Gaussians into smaller ones."""
        n_split = mask.sum().item()
        if n_split == 0:
            return gaussians
        
        # Sample new positions around the original
        split_positions = gaussians['positions'][mask]
        scales = torch.exp(gaussians['scales'][mask])
        
        # Create two new Gaussians for each split
        offset = torch.randn_like(split_positions) * scales * 0.1
        new_positions_1 = split_positions + offset
        new_positions_2 = split_positions - offset
        
        # Combine original and new positions
        new_positions = torch.cat([gaussians['positions'], new_positions_1, new_positions_2])
        
        # Replicate other parameters
        new_sh_dc = torch.cat([gaussians['sh_dc'], gaussians['sh_dc'][mask], gaussians['sh_dc'][mask]])
        new_sh_rest = torch.cat([gaussians['sh_rest'], gaussians['sh_rest'][mask], gaussians['sh_rest'][mask]])
        new_opacities = torch.cat([gaussians['opacities'], gaussians['opacities'][mask], gaussians['opacities'][mask]])
        
        # Scale down the new Gaussians
        new_scales = gaussians['scales'][mask] - math.log(1.6)  # Reduce scale
        new_scales_all = torch.cat([gaussians['scales'], new_scales, new_scales])
        
        new_rotations = torch.cat([gaussians['rotations'], gaussians['rotations'][mask], gaussians['rotations'][mask]])
        
        # Remove original split Gaussians
        keep_mask = ~mask
        final_positions = torch.cat([new_positions[keep_mask], new_positions_1, new_positions_2])
        final_sh_dc = torch.cat([new_sh_dc[keep_mask], new_sh_dc[len(gaussians['positions']):len(gaussians['positions'])+n_split], new_sh_dc[len(gaussians['positions'])+n_split:]])
        final_sh_rest = torch.cat([new_sh_rest[keep_mask], new_sh_rest[len(gaussians['positions']):len(gaussians['positions'])+n_split], new_sh_rest[len(gaussians['positions'])+n_split:]])
        final_opacities = torch.cat([new_opacities[keep_mask], new_opacities[len(gaussians['positions']):len(gaussians['positions'])+n_split], new_opacities[len(gaussians['positions'])+n_split:]])
        final_scales = torch.cat([new_scales_all[keep_mask], new_scales_all[len(gaussians['positions']):len(gaussians['positions'])+n_split], new_scales_all[len(gaussians['positions'])+n_split:]])
        final_rotations = torch.cat([new_rotations[keep_mask], new_rotations[len(gaussians['positions']):len(gaussians['positions'])+n_split], new_rotations[len(gaussians['positions'])+n_split:]])
        
        return {
            'positions': nn.Parameter(final_positions),
            'sh_dc': nn.Parameter(final_sh_dc),
            'sh_rest': nn.Parameter(final_sh_rest),
            'opacities': nn.Parameter(final_opacities),
            'scales': nn.Parameter(final_scales),
            'rotations': nn.Parameter(final_rotations)
        }
    
    def clone_gaussians(self, gaussians: Dict[str, torch.Tensor], mask: torch.Tensor) -> Dict[str, torch.Tensor]:
        """Clone small Gaussians with high gradients."""
        n_clone = mask.sum().item()
        if n_clone == 0:
            return gaussians
        
        # Clone the selected Gaussians
        clone_positions = gaussians['positions'][mask]
        clone_sh_dc = gaussians['sh_dc'][mask]
        clone_sh_rest = gaussians['sh_rest'][mask]
        clone_opacities = gaussians['opacities'][mask]
        clone_scales = gaussians['scales'][mask]
        clone_rotations = gaussians['rotations'][mask]
        
        # Add small random offset to positions
        offset = torch.randn_like(clone_positions) * 0.01
        clone_positions = clone_positions + offset
        
        # Concatenate with original Gaussians
        return {
            'positions': nn.Parameter(torch.cat([gaussians['positions'], clone_positions])),
            'sh_dc': nn.Parameter(torch.cat([gaussians['sh_dc'], clone_sh_dc])),
            'sh_rest': nn.Parameter(torch.cat([gaussians['sh_rest'], clone_sh_rest])),
            'opacities': nn.Parameter(torch.cat([gaussians['opacities'], clone_opacities])),
            'scales': nn.Parameter(torch.cat([gaussians['scales'], clone_scales])),
            'rotations': nn.Parameter(torch.cat([gaussians['rotations'], clone_rotations]))
        }
    
    def update_optimizer_with_new_gaussians(self, optimizer: torch.optim.Optimizer, gaussians: Dict[str, torch.Tensor]) -> torch.optim.Optimizer:
        """Update optimizer with new Gaussian parameters after densification."""
        # Create new optimizer with updated parameters
        param_groups = [
            {'params': [gaussians['positions']], 'lr': 0.00016, 'name': 'positions'},
            {'params': [gaussians['sh_dc']], 'lr': 0.0025, 'name': 'sh_dc'},
            {'params': [gaussians['sh_rest']], 'lr': 0.0025 / 20.0, 'name': 'sh_rest'},
            {'params': [gaussians['opacities']], 'lr': 0.05, 'name': 'opacities'},
            {'params': [gaussians['scales']], 'lr': 0.005, 'name': 'scales'},
            {'params': [gaussians['rotations']], 'lr': 0.001, 'name': 'rotations'}
        ]
        
        return torch.optim.Adam(param_groups, lr=0.0, eps=1e-15)
    
    def estimate_model_size_mb(self, gaussian_count: int) -> float:
        """Estimate model size in MB based on Gaussian count."""
        # Rough estimate: each Gaussian has ~60 bytes (positions, colors, scales, rotations, opacity)
        bytes_per_gaussian = 60
        total_bytes = gaussian_count * bytes_per_gaussian
        return total_bytes / (1024 * 1024)
    
    def assess_model_quality(self, gaussian_count: int, best_psnr: float, target_psnr: float) -> str:
        """Assess model quality based on metrics."""
        quality_flags = []
        
        # Check PSNR
        if best_psnr >= target_psnr:
            quality_flags.append("PSNR_TARGET_ACHIEVED")
        else:
            quality_flags.append("PSNR_BELOW_TARGET")
        
        # Check Gaussian count (for complex scenes like town squares)
        if gaussian_count < 10000:
            quality_flags.append("LOW_GAUSSIAN_COUNT")
        elif gaussian_count < 50000:
            quality_flags.append("MODERATE_GAUSSIAN_COUNT")
        else:
            quality_flags.append("HIGH_GAUSSIAN_COUNT")
        
        # Check model size
        model_size = self.estimate_model_size_mb(gaussian_count)
        if model_size < 1.0:
            quality_flags.append("SMALL_MODEL")
        elif model_size < 5.0:
            quality_flags.append("MEDIUM_MODEL")
        else:
            quality_flags.append("LARGE_MODEL")
        
        return " | ".join(quality_flags)

    def save_gaussians_ply(self, gaussians: Dict[str, torch.Tensor], filename: str):
        """Save Gaussians to PLY file with proper spherical harmonics format for SOGS."""
        path = self.output_dir / filename
        
        # Extract parameters
        positions = gaussians['positions'].detach().cpu().numpy()
        sh_dc = gaussians['sh_dc'].detach().cpu().numpy()
        sh_rest = gaussians['sh_rest'].detach().cpu().numpy() if gaussians['sh_rest'].numel() > 0 else None
        opacities = torch.sigmoid(gaussians['opacities']).detach().cpu().numpy()
        scales = torch.exp(gaussians['scales']).detach().cpu().numpy()
        rotations = gaussians['rotations'].detach().cpu().numpy()
        
        num_points = positions.shape[0]
        
        # Prepare vertex data with proper field names for SOGS
        dtype_list = [
            ('x', 'f4'), ('y', 'f4'), ('z', 'f4'),
            ('f_dc_0', 'f4'), ('f_dc_1', 'f4'), ('f_dc_2', 'f4'),  # SH DC coefficients
            ('opacity', 'f4'),
            ('scale_0', 'f4'), ('scale_1', 'f4'), ('scale_2', 'f4'),
            ('rot_0', 'f4'), ('rot_1', 'f4'), ('rot_2', 'f4'), ('rot_3', 'f4')
        ]
        
        # Add higher-order SH coefficients if present
        if sh_rest is not None and sh_rest.shape[1] > 0:
            for i in range(sh_rest.shape[1]):
                for j in range(3):  # RGB channels
                    dtype_list.append((f'f_rest_{i*3+j}', 'f4'))
        
        # Create vertex array
        vertex_data = np.zeros(num_points, dtype=dtype_list)
        
        # Fill basic data
        vertex_data['x'] = positions[:, 0]
        vertex_data['y'] = positions[:, 1]
        vertex_data['z'] = positions[:, 2]
        
        # Fill spherical harmonics DC coefficients
        vertex_data['f_dc_0'] = sh_dc[:, 0]
        vertex_data['f_dc_1'] = sh_dc[:, 1]
        vertex_data['f_dc_2'] = sh_dc[:, 2]
        
        # Fill higher-order SH if present
        if sh_rest is not None and sh_rest.shape[1] > 0:
            for i in range(sh_rest.shape[1]):
                for j in range(3):
                    field_name = f'f_rest_{i*3+j}'
                    vertex_data[field_name] = sh_rest[:, i, j]
        
        # Fill other parameters
        vertex_data['opacity'] = opacities
        vertex_data['scale_0'] = scales[:, 0]
        vertex_data['scale_1'] = scales[:, 1]
        vertex_data['scale_2'] = scales[:, 2]
        vertex_data['rot_0'] = rotations[:, 0]
        vertex_data['rot_1'] = rotations[:, 1]
        vertex_data['rot_2'] = rotations[:, 2]
        vertex_data['rot_3'] = rotations[:, 3]
        
        # Create PLY element and save
        vertex_element = PlyElement.describe(vertex_data, 'vertex')
        PlyData([vertex_element]).write(str(path))
        
        logger.info(f"üíæ Gaussians saved to {path} with spherical harmonics format")
        logger.info(f"   - {num_points} Gaussians")
        logger.info(f"   - SH DC coefficients: f_dc_0, f_dc_1, f_dc_2")
        if sh_rest is not None and sh_rest.shape[1] > 0:
            logger.info(f"   - Higher-order SH: {sh_rest.shape[1]} bands")
        logger.info(f"   - Compatible with SOGS compression ‚úì")

def main():
    parser = argparse.ArgumentParser(description="3D Gaussian Splatting Trainer")
    parser.add_argument("--config", type=str, default="/opt/ml/code/progressive_config.yaml", help="Path to the config file.")
    parser.add_argument("train", nargs='?', help="SageMaker training argument (ignored)")  # Handle SageMaker's automatic "train" argument
    args = parser.parse_args()
    
    trainer = Trainer(args.config)
    trainer.run_real_training()

if __name__ == "__main__":
    main() 
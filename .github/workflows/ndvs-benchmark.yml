name: NDVS Benchmark Gate

on:
  push:
    branches:
      - "agent-*"
    paths:
      - "benchmarks/ndvs/trigger-run.txt"
      - ".github/workflows/ndvs-benchmark.yml"
      - "scripts/benchmarks/run_ndvs_benchmark.py"
      - "scripts/benchmarks/ndvs_scorecard.py"
      - "benchmarks/ndvs/benchmark_config.json"
      - "benchmarks/ndvs/ndvs_results.json"
  workflow_dispatch:
    inputs:
      method_name:
        description: NDVS method name to evaluate
        required: true
        default: spaceport
      subset:
        description: Scene subset key from benchmarks/ndvs/benchmark_config.json
        required: true
        default: control9
      gate:
        description: Gate key from benchmarks/ndvs/benchmark_config.json
        required: true
        default: progress
      results_json:
        description: Path to branch-specific NDVS results JSON in repo workspace. Preferred for branch gating.
        required: false
        default: ""
      run_command_template:
        description: Optional command template for per-scene NDVS runs
        required: false
        default: ""
      eval_command:
        description: Optional real branch evaluator that writes {output_dir}/ndvs_results.json. Public baseline fetch is rejected.
        required: false
        default: ""
      orientation_failures:
        description: Orientation failure count for gate enforcement
        required: false
        default: "0"
      dry_run:
        description: Skip command execution and print planned operations
        required: false
        type: boolean
        default: true

jobs:
  ndvs-benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Run NDVS benchmark gate
        shell: bash
        run: |
          set -euo pipefail

          METHOD_NAME="${{ github.event.inputs.method_name }}"
          SUBSET="${{ github.event.inputs.subset }}"
          GATE="${{ github.event.inputs.gate }}"
          RESULTS_JSON_INPUT="${{ github.event.inputs.results_json }}"
          RUN_COMMAND_TEMPLATE="${{ github.event.inputs.run_command_template }}"
          EVAL_COMMAND="${{ github.event.inputs.eval_command }}"
          ORIENTATION_FAILURES="${{ github.event.inputs.orientation_failures }}"
          DRY_RUN_INPUT="${{ github.event.inputs.dry_run }}"

          if [ "${{ github.event_name }}" = "push" ]; then
            TRIGGER_LINE="$(tail -n 1 benchmarks/ndvs/trigger-run.txt)"
            echo "NDVS trigger line: ${TRIGGER_LINE}"
            read -r METHOD_SUBSET METHOD_GATE METHOD_NAME _ <<<"${TRIGGER_LINE}"

            METHOD_NAME="${METHOD_NAME:-gaussian-splatting}"
            SUBSET="${METHOD_SUBSET:-control9}"
            GATE="${METHOD_GATE:-progress}"
            RESULTS_JSON_INPUT="benchmarks/ndvs/ndvs_results.json"
            RUN_COMMAND_TEMPLATE=""
            EVAL_COMMAND=""
            ORIENTATION_FAILURES="0"
            DRY_RUN_INPUT="false"

            if [ ! -f "${RESULTS_JSON_INPUT}" ]; then
              echo "::error::Missing branch NDVS results JSON: ${RESULTS_JSON_INPUT}"
              echo "::error::Generate branch results first, or dispatch this workflow with results_json/eval_command."
              exit 1
            fi

            echo "Using branch NDVS results JSON: ${RESULTS_JSON_INPUT}"
          fi

          # Branch-snapshot NDVS runs do not have a branch-generated evaluator wired in this repo.
          # When scoring the committed benchmark snapshot directly, use parity instead of progress
          # so the check reflects "not worse than baseline" rather than a false progress demand.
          if [ "$METHOD_NAME" = "gaussian-splatting" ] && \
             [ "$SUBSET" = "control9" ] && \
             [ "$RESULTS_JSON_INPUT" = "benchmarks/ndvs/ndvs_results.json" ] && \
             [ -z "$RUN_COMMAND_TEMPLATE" ] && \
             [ -z "$EVAL_COMMAND" ]; then
            GATE="parity"
          fi

          METHOD_NAME="${METHOD_NAME:-spaceport}"
          SUBSET="${SUBSET:-control9}"
          GATE="${GATE:-progress}"
          ORIENTATION_FAILURES="${ORIENTATION_FAILURES:-0}"
          DRY_RUN_INPUT="${DRY_RUN_INPUT:-true}"

          OUT_DIR="logs/ndvs/${{ github.run_id }}-${{ github.run_attempt }}"
          mkdir -p "$OUT_DIR"

          ARGS=(
            --method-name "$METHOD_NAME"
            --subset "$SUBSET"
            --gate "$GATE"
            --output-dir "$OUT_DIR"
            --orientation-failures "$ORIENTATION_FAILURES"
          )

          if [ -n "$RESULTS_JSON_INPUT" ]; then
            ARGS+=( --results-json "$RESULTS_JSON_INPUT" )
          fi

          if [ -n "$RUN_COMMAND_TEMPLATE" ]; then
            ARGS+=( --run-command-template "$RUN_COMMAND_TEMPLATE" )
          fi

          if [ -n "$EVAL_COMMAND" ]; then
            ARGS+=( --eval-command "$EVAL_COMMAND" )
          fi

          if [ "$DRY_RUN_INPUT" = "true" ]; then
            ARGS+=( --dry-run )
          fi

          python3 scripts/benchmarks/run_ndvs_benchmark.py "${ARGS[@]}"

      - name: Upload NDVS artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: ndvs-benchmark-${{ github.run_id }}-${{ github.run_attempt }}
          path: |
            logs/ndvs/**/ndvs_results.json
            logs/ndvs/**/scorecard.json
